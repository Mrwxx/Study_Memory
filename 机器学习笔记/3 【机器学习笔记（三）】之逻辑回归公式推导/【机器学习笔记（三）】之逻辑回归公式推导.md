## 本文章由公号【开发小鸽】发布！欢迎关注！！！
<br>

**老规矩--妹妹镇楼：**
<center>
<img src="https://img-blog.csdnimg.cn/20200721223424816.JPG"   width="20%">

## 一. 逻辑回归
 &nbsp;  &nbsp;  &nbsp;  &nbsp;**目的**：经典的二分类算法
 &nbsp;  &nbsp;  &nbsp;  &nbsp;**机器学习算法选择**：先逻辑回归再用复杂的，越简单越好。
 &nbsp;  &nbsp;  &nbsp;  &nbsp;**逻辑回归的决策边界**：可以是非线性的。

## 二. Sigmoid函数
### (1). 公式：
$$ g(z) = \frac{1}{1 + e^{-z}}$$

 &nbsp;  &nbsp;  &nbsp;  &nbsp;逻辑回归所需要的Sigmoid函数，用来对预测值进行分类。自变量取值为任意实数，值域[0,1]
### (2). 解释：
 &nbsp;  &nbsp;  &nbsp;  &nbsp;将任意的输入映射到了[0,1]区间，我们在线性回归中可以得到一个**预测值**，再将该值**映射到Sigmoid函数中**，这样就完成了从值到概率的转换，也就是分类任务。Sigmoid函数是逻辑回归的重点。

### (3). 预测函数：
 &nbsp;  &nbsp;  &nbsp;  &nbsp;从输入中计算预测值的函数
$$ h_{\theta}(x) = g(\theta^{T}x) = \frac{1}{1+e^{-\theta^{T}x}}$$$$ \theta_{0} + \theta_{1}x_{1} + ... + \theta_{n}x_{n} = \sum_{i=1}^{n}\theta_{i}x_{i} = \theta^{T}x $$

 &nbsp;  &nbsp;  &nbsp;  &nbsp;可以看到，预测函数直接将输入值作为Sigmoid函数的参数，得出的结果直接转换到[0,1]区间上。
 &nbsp;  &nbsp;  &nbsp;  &nbsp;对于**二分类任务**：
$$ P(y=1 | x;\theta) = h_{\theta}(x)$$$$P(y=0|x;\theta) = 1 - h_{\theta}(x) $$

 &nbsp;  &nbsp;  &nbsp;  &nbsp;正例与反例的概率如上式所示，通过y取值0或1将上式概率整合为下式：
$$ P(y|x;\theta) = (h_{\theta}(x))^{y}(1-h_{\theta}(x))^{1-y}$$

<br>**思考**：我们要求的是θ这个参数，所以需要使用**似然函数**。式子如下：
$$ L(\theta) = \prod_{i=1}^{m}P(y_{i}|x_{i}; \theta) = \prod_{i=1}^{m}(h_{\theta}(x_{i}))^{y_{i}}(1-h_{\theta}(x_{i}))^{1-y_{i}}$$

 &nbsp;  &nbsp;  &nbsp;  &nbsp;m是样本数，将似然函数求对数，式子如下：
$$ l(\theta) = logL(\theta) = \sum_{i=1}^{m}(y_{i}logh_{\theta}(x_{i})+ (1-y_{i})log(1-h_{\theta}(x_{i})))$$

 &nbsp;  &nbsp;  &nbsp;  &nbsp;**连乘转换为连加，计算更简便。**
 <br>
 &nbsp;  &nbsp;  &nbsp;  &nbsp;**思考**：似然函数求解的是极大值，应用的是梯度上升来求取。而我们通常的求解套路是求解极小值，应用梯度下降求解，**那么如何将求解极大值转换为求解极小值呢**？例如，要求一个正的极大值，就相当于求一个负的极小值。因此，可以为上式**引入一个负数**。m是为了求取均值。
$$ J(\theta) = -\frac{1}{m}l(\theta)$$

 &nbsp;  &nbsp;  &nbsp;  &nbsp;转换为求解极小值后，就**转换为了梯度下降的求解问题**，一步步寻找下降方向。那么，就对θ进行求偏导，注意，这里的θ是有多少个θ就求多少次。
![在这里插入图片描述](https://img-blog.csdnimg.cn/20200618210304151.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L01yd3h4eHg=,size_16,color_FFFFFF,t_70)
**注意**

 &nbsp;  &nbsp;  &nbsp;  &nbsp;x~i~^j^  **指的是第i个样本的第j个特征**
 &nbsp;  &nbsp;  &nbsp;  &nbsp;求出来梯度下降的方向后，就可以根据步长对参数θ进行更新了：
$$ \theta_{j} := \theta_{j} - \alpha \frac{1}{m}\sum_{i=1}^{m}(h_{\theta}(x_{i})- y_{i})x_{i}^{j}$$

 &nbsp;  &nbsp;  &nbsp;  &nbsp;上式的后部分式子为**步长乘以梯度方向**，这是参数θ更新的部分。找到了参数的更新方法后，就可以用梯度下降的方法来寻找我们的极小值。

！
