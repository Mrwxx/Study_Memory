
## 本文章由公号【开发小鸽】发布！欢迎关注！！！
<br>

**老规矩--妹妹镇楼：**
<center>
<img src="https://img-blog.csdnimg.cn/20200721223424816.JPG"   width="20%">

## 一. 梯度下降
 &nbsp;  &nbsp;  &nbsp;  &nbsp;我们的**目标**是预测值与真实值的差距越小越好，由此可以设置**目标函数**为：$$ J(\theta) = \frac{1}{2m}\sum_{i-1}^{m}(y^{i} - h_{0}(x^{i}))^{2}$$
 &nbsp;  &nbsp;  &nbsp;  &nbsp;上式中，平方项内为真实值与预测值的差值，目标函数为差值的平方的均值。
 &nbsp;  &nbsp;  &nbsp;  &nbsp;**思考**：目标函数值越小越好，即求取目标函数的最小值点。通过梯度下降一点一点地求取。
### 1.批量梯度下降
$$ \frac{\partial J(\theta)}{\partial \theta_{j}} = -\frac{1}{m}\sum_{i=1}^{m}(y^{i} - h_{\theta}(x^{i}))x_{j}^{i}
$$$$\theta_{j}^{'} = \theta_{j} + \frac{1}{m}\sum_{i=1}^{m}(y^{i} - h_{\theta}(x^{i}))x_{j}^{i} $$

 &nbsp;  &nbsp;  &nbsp;  &nbsp;上式第一个式子为目标函数对θ求偏导，求得均值梯度，上式第二个式子即利用左边计算出的梯度值。
**优缺点**：容易得到最优解，但是由于每次计算需考虑所有样本，计算速度很慢。
### 2.随机梯度下降
$$ \theta_{j}^{'} = \theta_{j}+(y^{i} - h_{\theta}(x^{i}))x_{j}^{i}$$

 &nbsp;  &nbsp;  &nbsp;  &nbsp;随机寻找一个样本，计算梯度。
 &nbsp;  &nbsp;  &nbsp;  &nbsp;**优缺点**：迭代速度快，但不一定每次都朝着收敛的方向。
### 3.小批量梯度下降法
$$ \theta_{j}:= \theta_{j} - \alpha \frac{1}{10}\sum_{k=i}^{i+9}(h_{\theta}(x^{(k)}) - y^{(k)})x_{j}^{(k)}$$

 &nbsp;  &nbsp;  &nbsp;  &nbsp;每次更新选择一小部分数据来算。
 &nbsp;  &nbsp;  &nbsp;  &nbsp;**优缺点**：速度折中，方向准确度折中。

## 二. 学习率
 &nbsp;  &nbsp;  &nbsp;  &nbsp;即**步长**，每次前进的距离，一般设置的很小，如0.001。
**如何设置**？
 &nbsp;  &nbsp;  &nbsp;  &nbsp;一般从小的值开始，如果效果不好，再取消一些。


