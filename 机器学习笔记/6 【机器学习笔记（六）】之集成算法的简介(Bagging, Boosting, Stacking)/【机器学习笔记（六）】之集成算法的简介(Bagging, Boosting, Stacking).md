## 本文章由公号【开发小鸽】发布！欢迎关注！！！
<br>

**老规矩--妹妹镇楼：**
<center>
<img src="https://img-blog.csdnimg.cn/20200721223424816.JPG"   width="20%">

## 集成算法(Ensemble learning)
### 一. 目的：
  &nbsp; &nbsp; &nbsp; &nbsp;   用多个算法进行集成，让机器学习效果比单个算法的效果更好。
### 二. 类别：
#### (1). Bagging:
 &nbsp;  &nbsp;  &nbsp;  &nbsp; 训练多个分类器取**平均** 
$$ f(x) = 1/M\sum_{m=1}^{M}f_{m}(x)$$

#### (2). Boosting：
 &nbsp;  &nbsp;  &nbsp;  &nbsp; 从弱学习器开始**加强**，通过**加权**进行训练（不断添加新的树，完善前面的结果）
$$ F_{m}(x) = F_{m-1}(x) + argmin_{h}\sum_{i=1}^{n}L(y_{i},F_{m-1}(x_{i})+h(x_{i}))$$

#### (3). Stacking：
 &nbsp;  &nbsp;  &nbsp;  &nbsp; **聚合**多个分类或回归模型（分阶段）


下面具体来说明三种集成算法。

### 三. Bagging 模型
#### (1). 定义：
 &nbsp;  &nbsp;  &nbsp;  &nbsp; bootstrap aggregation 。并行训练很多分类器。
#### (2). 典型代表：
 &nbsp;  &nbsp;  &nbsp;  &nbsp; **随机森林**。所谓随机森林可以从森林与随机两部分来理解，森林即多棵树，即多个分类器是**并行**的；随机即是多个分类器的数据采样是随机的，特征的选择是随机的，因为多个分类器如果数据采样，特征选择都是相同的，那么结果也必定是相同的，这样就没有意义了。随机是为了**提高分类器的泛化能力**。
#### (3). 随机森林的优势：
1. 能够处理很**高维度的数据**，即特征很多的数据，且不用做特征选择。
2. 能够计算出**特征的重要程度**，给出哪些特征比较重要。
3. 多种分类器是**并行**的，运行速度快。
4. 由于神经网络是黑盒，无法得知网络运算过程中的具体情况。而随机森林是可以进行**可视化展示**，便于分析。

### 四. Boosting 模型
#### (1). 定义：
 &nbsp;  &nbsp;  &nbsp;  &nbsp; 从弱学习器开始加强，通过**加权**进行训练（不断添加新的树，完善前面的结果）
#### (2). 典型代表：
 &nbsp;  &nbsp;  &nbsp;  &nbsp; **AdaBoost, Xgboost**
#### (3). AdaBoost:
 &nbsp;  &nbsp;  &nbsp;  &nbsp; AdaBoost会根据前一次的分类效果**调整数据的权重**。即每次分类后，都会有分类正确，分类错误的情况，因此下一次分类要更加重视分类错误的情况，于是提高当前分类错误的数据的权重，减小分类正确的数据的权重。
#### (4). 结果：
 &nbsp;  &nbsp;  &nbsp;  &nbsp; 每个分类器的结果都不一样，有的准确率高，有的准确率低，高的分类器权重高，低的分类器权重低，将多个分类器根据权值进行合体，得到最终的boost结果。

### 五. Stacking 模型
#### (1). 定义：
 &nbsp;  &nbsp;  &nbsp;  &nbsp; 聚合多个分类或回归模型（**分阶段**）。即将各种分类器都堆在一起，通过各种方式得出结果。有的直接求均值，有的分阶段出结果。
#### (2). 分阶段：
 &nbsp;  &nbsp;  &nbsp;  &nbsp; **第一阶段通过各种分类器求出各自相应的分类结果，第二阶段再将第一阶段的结果作为输入，进行训练得出结果。**
#### (3). 效果：
 &nbsp;  &nbsp;  &nbsp;  &nbsp; 将多种分类器堆叠在一起会使**准确率有所提高**，但是**速度也会相应的减慢**。


