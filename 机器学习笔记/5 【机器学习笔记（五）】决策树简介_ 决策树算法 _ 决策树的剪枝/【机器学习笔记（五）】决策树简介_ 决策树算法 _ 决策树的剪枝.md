## 本文章由公号【开发小鸽】发布！欢迎关注！！！
<br>

**老规矩--妹妹镇楼：**
<center>
<img src="https://img-blog.csdnimg.cn/20200721223424816.JPG"   width="20%">

### 一. 决策树：
1. 从根节点开始一步步走到叶子节点（决策）
2. 所有数据最终都会落到叶子节点，既可以做分类也可以做回归

### 二. 决策树的组成：
1. **根节点**：第一个选择点
2. **非叶子节点与分支**：中间过程
3. **叶子节点**：最终的决策结果

### 三. 决策树的训练与测试
1. **训练阶段**：从给定的训练集构造出来一颗树（从根节点开始选择特征）
2. **测试阶段**：根据构造出来的树模型从根节点开始用测试集测试
3. 难点在于如何构造一个树，特征分类的选择问题

### 四. 如何选择特征节点呢？
#### (1). 目标：
&nbsp;  &nbsp;  &nbsp;  &nbsp;通过一种衡量标准，计算通过不同特征进行分支选择后的分类情况，找出情况最好的那个特征作为分支节点。

#### (2). 衡量标准——熵
&nbsp;  &nbsp;  &nbsp;  &nbsp;熵：熵是表示随机变量不确定性的度量，即物体内部的混乱程度。
&nbsp;  &nbsp;  &nbsp;  &nbsp;公式：
$$H(X) = -\sum pi * log pi, i = 1, 2, ...n$$

&nbsp;  &nbsp;  &nbsp;  &nbsp;**注意**公式中的负号，由于pi是概率，在0-1之间，因此log pi 必为非正数，当概率pi越大，越接近于1，则log pi 越接近于0；反之，pi越小，越接近于0，则log pi 越接近于1；

&nbsp;  &nbsp;  &nbsp;  &nbsp;**熵**：不确定性越大，则得到的熵值也就越大。
当p = 0 或 p = 1 时，H(p) = 0, 随机变量完全没有不确定性
当p = 0.5时，H(p) = 1,此时随机变量的不确定性最大

### 五. 如何判断选择一个特征作为节点的好坏呢？
&nbsp;  &nbsp;  &nbsp;  &nbsp;用信息增益判断，信息增益越大，说明选择此特征的效果越好。
&nbsp;  &nbsp;  &nbsp;  &nbsp;**信息增益**：表示特征X使得类Y的不确定性减少的程度。（分类后的专一性，希望分类后的结果是同类在一起，混乱程度降低）

### 六. 决策树算法：
1. **ID3算法**：
&nbsp;  &nbsp;  &nbsp;  &nbsp;信息增益，例如像ID这样的特征，若ID为0-14这14个数，每个ID的熵都为0，此时的熵为0，则意味着此时的信息增益是最大的，然而ID只是一个编号，对于解决问题没有帮助。这就是这个算法的问题，有的特征本身非常稀疏，里面的属性很多，但是每个属性里面的样本数很小，这样就容易计算出很小的熵值，信息增益就会计算得很大，但这是无意义的。

2. **C4.5**：
&nbsp;  &nbsp;  &nbsp;  &nbsp;信息增益率，在信息增益的基础上，考虑自身的熵值，即信息增益比上自身的熵值，若本身的熵值就很大，那么在信息增益比上本身的熵值后，信息增益率就会很小，不会产生ID3算法的误解。
3. **CART**：
&nbsp;  &nbsp;  &nbsp;  &nbsp;使用GINI系数来当做衡量标准
5. **GINI系数**：$$
Gini(p) = \sum_{k=1}^{k}p_{k}(1- p_{k}) = 1 - \sum_{k=1}^{k}p_{k}^{2}$$ 
&nbsp;  &nbsp;  &nbsp;  &nbsp;和熵的衡量标准类似，计算方式不相同。
### 七. 决策树的剪枝策略
#### (1). 为什么要剪枝？
&nbsp;  &nbsp;  &nbsp;  &nbsp;特征量过大，由于每次都要用剩余的特征来寻找**最优信息增益**，导致决策树过于庞大，决策树**过拟合**风险过大。即在训练集上表现非常好，但在测试集中表现很差。

#### (2). 剪枝策略：
1. **预剪枝**：
&nbsp;  &nbsp;  &nbsp;  &nbsp;边建立决策树边进行剪枝。（更实用）
	限制深度（特征数），叶子节点个数，叶子节点样本数，信息增益量等
2. **后剪枝**：
&nbsp;  &nbsp;  &nbsp;  &nbsp;当建立完决策树后进行剪枝操作。
	通过一定的衡量标准
	$$C_{\alpha }(T) = C(T) + \alpha \cdot \left | T_{leaf} \right |$$
	
	&nbsp;  &nbsp;  &nbsp;  &nbsp;C(T)为当前节点损失，当前节点的所有叶子节点的样本数乘以基尼系数的和。
	 &nbsp;  &nbsp;  &nbsp;  &nbsp;后剪枝限制叶子节点的个数，**Tleaf**为叶子节点个数，即叶子节点越多，损失越大。
	&nbsp;  &nbsp;  &nbsp;  &nbsp;上式用于判断节点是否需要分裂，先计算不分裂时的损失值，再计算分裂时的损失值，判断哪个损失小，选择哪种处理方式：分裂或不分裂。


